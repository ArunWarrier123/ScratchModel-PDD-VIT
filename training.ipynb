{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import wandb\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patchify import patchify\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from vit import ViT\n",
    "# from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wandb' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wandb login d9ae8f452cf7f0c4ed803a7364869721e5dd2f52\n",
    "# token = d9ae8f452cf7f0c4ed803a7364869721e5dd2f52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['RoughBark', 'StripeCanker']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = './Dataset/'  # Replace with the path to your dataset folder\n",
    "\n",
    "# Get the class names from the subfolder names\n",
    "class_names = [class_name for class_name in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, class_name))]\n",
    "\n",
    "# Print the class names\n",
    "print(len(class_names))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "hp = {}\n",
    "hp[\"image_size\"] = 200\n",
    "hp[\"num_channels\"] = 3\n",
    "hp[\"patch_size\"] = 25\n",
    "hp[\"num_patches\"] = (hp[\"image_size\"]**2) // (hp[\"patch_size\"]**2)\n",
    "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"]*hp[\"patch_size\"]*hp[\"num_channels\"])\n",
    "\n",
    "hp[\"batch_size\"] = 32\n",
    "hp[\"lr\"] = 1e-4\n",
    "hp[\"num_epochs\"] = 2\n",
    "hp[\"num_classes\"] = len(class_names)\n",
    "hp[\"class_names\"] = class_names\n",
    "\n",
    "hp[\"num_layers\"] = 12\n",
    "hp[\"hidden_dim\"] = 768\n",
    "hp[\"mlp_dim\"] = 3072\n",
    "hp[\"num_heads\"] = 12\n",
    "hp[\"dropout_rate\"] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "    split=0.15\n",
    "    # Load training data\n",
    "    train_images = shuffle(glob(os.path.join(train_path, \"*\", \"*.jpg\")))\n",
    "    \n",
    "    # subdirectories = glob(os.path.join(train_path, \"*\"))\n",
    "    # print({\"subdir\" : subdirectories})\n",
    "    # train_images = shuffle(glob(os.path.join(train_path, \"*\", \"*.jpg\")))\n",
    "    # train_images = shuffle(glob(os.path.join(train_path, \"*\", \"*.jpg\"), recursive=True))\n",
    "\n",
    "    # images_array = os.listdir(\"/content/drive/MyDrive/Datasets/VerySmallDataset/train/RoughBark\")\n",
    "\n",
    "    # print(subdirectories)\n",
    "    # Collect images from each subdirectory\n",
    "    # train_images = []\n",
    "    # images_array = []\n",
    "    # for subdir in subdirectories:\n",
    "    #   # print(subdir)\n",
    "    #   images_array.extend(os.listdir(subdir))\n",
    "\n",
    "    # train_images = shuffle(images_array)\n",
    "\n",
    "    # print(train_images)\n",
    "    split_size = int(len(train_images) * split)\n",
    "    train_x, valid_x = train_test_split(train_images, test_size=split_size, random_state=42)\n",
    "\n",
    "    # Load test data from a different folder\n",
    "    test_x = shuffle(glob(os.path.join(test_path, \"*\", \"*.jpg\")))\n",
    "\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_label(path):\n",
    "    \"\"\" Reading images \"\"\"\n",
    "    path = path.decode()\n",
    "    print(path)\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
    "    image = image/255.0\n",
    "\n",
    "    \"\"\" Preprocessing to patches \"\"\"\n",
    "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
    "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
    "\n",
    "    # patches = np.reshape(patches, (64, 25, 25, 3))\n",
    "    # for i in range(64):\n",
    "    #     cv2.imwrite(f\"files/{i}.png\", patches[i])\n",
    "\n",
    "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
    "    patches = patches.astype(np.float32)\n",
    "\n",
    "    \"\"\" Label \"\"\"\n",
    "    # print({\"path\": path})\n",
    "    # class_name = path.split(\"/\")[-2]\n",
    "    # print({\"classname\": class_name})\n",
    "    normalized_path = os.path.normpath(path)\n",
    "\n",
    "    # Split the normalized path\n",
    "    path_parts = normalized_path.split(os.sep)\n",
    "\n",
    "    # Extract the second-to-last part as the class name\n",
    "    class_name = path_parts[-2]\n",
    "\n",
    "    # print(class_name)\n",
    "    class_idx = hp[\"class_names\"].index(class_name)\n",
    "    class_idx = np.array(class_idx, dtype=np.int32)\n",
    "\n",
    "    return patches, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
    "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
    "\n",
    "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
    "    labels.set_shape(hp[\"num_classes\"])\n",
    "\n",
    "    return patches, labels\n",
    "\n",
    "def tf_dataset(images, batch=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Seeding \"\"\"\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\"\"\" Directory for storing files \"\"\"\n",
    "create_dir(\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 278 - Valid: 48 - Test: 326\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Paths \"\"\"\n",
    "train_path = \"./Dataset/\"\n",
    "test_path = \"./Dataset/\"\n",
    "model_path = os.path.join(\"files\", \"model_vms.h5\")\n",
    "csv_path = os.path.join(\"files\", \"log_vms.csv\")\n",
    "\n",
    "\"\"\" Dataset \"\"\"\n",
    "train_x, valid_x, test_x = load_data(train_path, test_path)\n",
    "print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf_dataset(train_x, batch=hp[\"batch_size\"])\n",
    "valid_ds = tf_dataset(valid_x, batch=hp[\"batch_size\"])\n",
    "\n",
    "\"\"\" Model \"\"\"\n",
    "model = ViT(hp)\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n",
    "    metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\MajorProject\\PlantDiseaseDetectionVIT\\training.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MajorProject/PlantDiseaseDetectionVIT/training.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wandb\u001b[39m.\u001b[39minit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "./Dataset\\StripeCanker\\IMG_1573.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4862.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4815.JPG\n",
      "./Dataset\\RoughBark\\IMG_1965.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1626.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4807.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4837.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4886.JPG\n",
      "./Dataset\\RoughBark\\IMG_4783.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4806.JPG\n",
      "./Dataset\\RoughBark\\IMG_2058.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1808.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4805.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4876.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4889.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4810.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1679.JPG\n",
      "./Dataset\\RoughBark\\IMG_4862.JPG\n",
      "./Dataset\\RoughBark\\IMG_4853.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4860.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4788.JPG\n",
      "./Dataset\\RoughBark\\IMG_4813.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1779.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4888.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1622.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1645.JPG\n",
      "./Dataset\\StripeCanker\\IMG_4799.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4844.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1654.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1554.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1611.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4859.JPG\n",
      "./Dataset\\RoughBark\\IMG_2044.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1636.JPG\n",
      "./Dataset\\RoughBark\\IMG_4812.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1638.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4880.JPG\n",
      "./Dataset\\RoughBark\\IMG_4830.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4793.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4883.JPG\n",
      "./Dataset\\RoughBark\\IMG_4861.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1628.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1875.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1629.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4875.JPG\n",
      "./Dataset\\RoughBark\\IMG_4829.JPG\n",
      "./Dataset\\RoughBark\\IMG_4832.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1705.JPG\n",
      "./Dataset\\StripeCanker\\IMG_4877.JPG\n",
      "./Dataset\\StripeCanker\\IMG_E4820.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4836.JPG\n",
      "./Dataset\\RoughBark\\IMG_1563.JPG\n",
      "./Dataset\\RoughBark\\IMG_4807.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1651.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1557.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1667.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4798.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1624.JPG\n",
      "./Dataset\\StripeCanker\\IMG_4879.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1623.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1783.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4861.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1607.JPG\n",
      "./Dataset\\RoughBark\\IMG_E4783.JPG\n",
      "./Dataset\\RoughBark\\IMG_4796.JPG\n",
      "./Dataset\\StripeCanker\\IMG_4888.JPG\n",
      "./Dataset\\StripeCanker\\IMG_1556.JPG\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
    "    CSVLogger(csv_path),\n",
    "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
    "    # WandbCallback(),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=hp[\"num_epochs\"],\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3107865d8f664eff80ef8560b0104544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-valley-2</strong> at: <a href='https://wandb.ai/model-analyze/ScratchModel-PDD-VIT/runs/bx022w2x' target=\"_blank\">https://wandb.ai/model-analyze/ScratchModel-PDD-VIT/runs/bx022w2x</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231113_234741-bx022w2x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
